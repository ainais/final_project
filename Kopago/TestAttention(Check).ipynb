{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소켓을 사용하기 위해서는 socket을 import해야 한다.\n",
    "import socket, threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, ko_vocab_size))\n",
    "    target_seq[0, 0, ko_idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx_to_ko[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_ko_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, ko_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "filename = 'kor-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)  \n",
    "lines = pd.read_csv('kor.txt', names=['en','ko'], sep='\\t', index_col=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 915)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 2048), 24281088    input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 915)    1874835     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 26,155,923\n",
      "Trainable params: 26,155,923\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-----------------------------------\n",
      "입력 문장: Say hello.\n",
      "정답 문장:  인 \n",
      "번역기가 번역한 문장:  인사해. \n"
     ]
    }
   ],
   "source": [
    "      lines.en[0]= \"Say hello.\"\n",
    "      lines.en[0]\n",
    "      lines.ko[0]=\"인\"\n",
    "      lines.ko[0]  \n",
    "      lines.ko = lines.ko.apply(lambda x: '\\t '+x+' \\n')  \n",
    "      # en & ko 글자 집합 구축\n",
    "      en_vocab = set()\n",
    "      for line in lines.en :\n",
    "        for char in line:\n",
    "          en_vocab.add(char)\n",
    "\n",
    "      ko_vocab = set()\n",
    "      for line in lines.ko :\n",
    "        for char in line:\n",
    "          ko_vocab.add(char)\n",
    "        \n",
    "      en_vocab_size = len(en_vocab)+1\n",
    "      ko_vocab_size = len(ko_vocab)+1\n",
    "        \n",
    "      en_vocab = sorted(list(en_vocab))    \n",
    "      ko_vocab = sorted(list(ko_vocab))  \n",
    "        \n",
    "      en_idx = dict([(word, i+1) for i, word in enumerate(en_vocab)])\n",
    "      ko_idx = dict([(word, i+1) for i, word in enumerate(ko_vocab)])  \n",
    "      \n",
    "      # 입력 구성\n",
    "      encoder_input = []\n",
    "      for line in lines.en :\n",
    "        temp_X = []\n",
    "        for w in line:\n",
    "          temp_X.append(en_idx[w])   #char - int 변환\n",
    "        encoder_input.append(temp_X)\n",
    "      \n",
    "      # 출력 구성\n",
    "      decoder_input = []\n",
    "      for line in lines.ko :\n",
    "        temp_X = []\n",
    "        for w in line:\n",
    "          temp_X.append(ko_idx[w])   #char - int 변환\n",
    "        decoder_input.append(temp_X)\n",
    "      # ko column 의 \\t 제거\n",
    "      decoder_target = []\n",
    "      for line in lines.ko :\n",
    "        t=0\n",
    "        temp_X = []\n",
    "        for w in line:\n",
    "          if t>0:\n",
    "            temp_X.append(ko_idx[w])\n",
    "          t=t+1\n",
    "        decoder_target.append(temp_X)\n",
    "    \n",
    "      # max length\n",
    "      max_en_len = max([len(line) for line in lines.en])\n",
    "      max_ko_len = max([len(line) for line in lines.ko])\n",
    "        \n",
    "      # 최대 길이를 10으로 나누어 사용 (학습 시간 단축)\\n\",\n",
    "      max_en_len //= 10\n",
    "      max_ko_len //= 10\n",
    "\n",
    "      encoder_input = pad_sequences(encoder_input, maxlen=max_en_len, padding='post')\n",
    "      decoder_input = pad_sequences(decoder_input, maxlen=max_ko_len, padding='post')\n",
    "      decoder_target = pad_sequences(decoder_target, maxlen=max_ko_len, padding='post')\n",
    "        \n",
    "      # 원핫 벡터\n",
    "      encoder_input = to_categorical(encoder_input)\n",
    "      decoder_input = to_categorical(decoder_input)\n",
    "      decoder_target = to_categorical(decoder_target)\n",
    "      \n",
    "      encoder_inputs = Input(shape=(None, en_vocab_size))\n",
    "      encoder_lstm = LSTM(units=2048, return_state=True)\n",
    "      encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "      encoder_states = [state_h, state_c] #은닉 상태, 셀 상태\n",
    "      \n",
    "      decoder_inputs = Input(shape=(None, ko_vocab_size)) \n",
    "      decoder_lstm = LSTM(units=2048, return_sequences=True, return_state=True)\n",
    "      decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "      decoder_softmax_layer = Dense(ko_vocab_size, activation='softmax')\n",
    "      decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "      # embedding 계층 관련 학습!\n",
    "      model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "      model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "      model.load_weights('./kopago_model.hdf5')\n",
    "      encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "        \n",
    "      # 이전 시점의 상태들을 저장하는 텐서\n",
    "      decoder_state_input_h = Input(shape=(2048,))\n",
    "      decoder_state_input_c = Input(shape=(2048,))\n",
    "      decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "      decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "      # 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "      decoder_states = [state_h, state_c]\n",
    "      # 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "      decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "      decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "      decoder_model.summary()  \n",
    "    \n",
    "      idx_to_en = dict((i, char) for char, i in en_idx.items())\n",
    "      idx_to_ko = dict((i, char) for char, i in ko_idx.items())\n",
    "    \n",
    "      seq_index = 0\n",
    "      input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "      decoded_sentence = decode_sequence(input_seq)\n",
    "      print(35 * \"-\")\n",
    "      print('입력 문장:', lines.en[0])\n",
    "      print('정답 문장:', lines.ko[0][1:len(lines.ko[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "      print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
